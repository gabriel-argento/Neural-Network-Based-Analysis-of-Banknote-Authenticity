{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-5Hl-j-4pxC"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler #normalisation\n",
    "from sklearn.metrics import accuracy_score #scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f00PSFOP4U6G"
   },
   "outputs": [],
   "source": [
    "# Euclidean distance function\n",
    "def euclidean_distance(v1, v2):\n",
    "    return np.linalg.norm(v1 - v2)\n",
    "\n",
    "# Find the Best Matching Unit (BMU)\n",
    "def find_bmu(input_vector, weights):\n",
    "    bmu_idx = None\n",
    "    min_dist = np.inf\n",
    "\n",
    "    # Compare with each neuron\n",
    "    for i in range(map_width):\n",
    "        for j in range(map_height):\n",
    "            w = weights[i, j]\n",
    "            dist = euclidean_distance(input_vector, w)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                bmu_idx = (i, j)\n",
    "\n",
    "    return bmu_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oWwIboD34EH"
   },
   "outputs": [],
   "source": [
    "# Mexican hat function for calculating influence\n",
    "def mexican_hat_function(dist_to_bmu, radius):\n",
    "    gaussian_part = np.exp(-dist_to_bmu**2 / (2 * radius**2))  # Excitation (Gaussian part)\n",
    "    inhibitory_part = np.exp(-dist_to_bmu**2 / (radius**2))  # Inhibition part\n",
    "    return gaussian_part - inhibitory_part  # Excitation minus inhibition\n",
    "\n",
    "\n",
    "# Update neuron weights using the Mexican Hat function\n",
    "def update_weights_mexican_hat(input_vector, bmu_idx, iteration, weights, map_width, map_height, learning_rate, time_constant):\n",
    "    # Calculate neighborhood radius with exponential decay\n",
    "    radius = initial_radius * np.exp(-iteration / time_constant)\n",
    "\n",
    "    # Update weights of neurons within BMU's neighborhood\n",
    "    for i in range(map_width):\n",
    "        for j in range(map_height):\n",
    "            w = weights[i, j]\n",
    "            dist_to_bmu = euclidean_distance(np.array([i, j]), np.array(bmu_idx))\n",
    "\n",
    "            # If the neuron is within the neighborhood\n",
    "            if dist_to_bmu < radius:\n",
    "                # Calculate influence using the Mexican Hat function\n",
    "                influence = mexican_hat_function(dist_to_bmu, radius)\n",
    "                # Update weights based on calculated influence\n",
    "                weights[i, j] += influence * learning_rate * (input_vector - w)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "    # Visualization of the Kohonen map weights (1D projection)\n",
    "def visualize_som_weights(weights):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights[:, :, 0], cmap=\"viridis\", annot=False, cbar=True)\n",
    "    plt.title(\"Kohonen Map (SOM) - Projection of Weights (Dimension 1)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function for inference with a new input vector\n",
    "def infer_som(input_vector, weights):\n",
    "    # Find the BMU for the new input vector\n",
    "    bmu_idx = find_bmu(input_vector, weights)\n",
    "    return bmu_idx\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the map with multiple BMUs\n",
    "def visualize_bmu_multiple(bmu_indices, weights, sample_classes):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights[:, :, 0], cmap=\"viridis\", annot=False, cbar=True)\n",
    "    plt.title(\"Kohonen Map (SOM) with BMUs for Multiple Samples\")\n",
    "\n",
    "    # Add markers for each BMU\n",
    "    for idx, bmu_idx in enumerate(bmu_indices):\n",
    "        #plt.scatter(bmu_idx[1] + 0.5, bmu_idx[0] + 0.5, s=200, c='red', marker='X')\n",
    "        plt.text(bmu_idx[1] + 0.5, bmu_idx[0] + 0.5, f\"C: {sample_classes[idx]}\", color='white', fontsize=12, ha='center', va='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function to test inference on random samples\n",
    "def test_inference(X, y, weights):\n",
    "    # Select n random indices from the dataset\n",
    "    #sample_indices = np.random.choice(range(len(X)), size=n_samples, replace=False)\n",
    "\n",
    "    bmu_indices = []\n",
    "    sample_classes = []\n",
    "\n",
    "    # For each sample, perform inference and store the BMUs and classes\n",
    "    #for idx in sample_indice:\n",
    "    for idx in range(0,X.shape[0]):\n",
    "        new_data = X[idx]\n",
    "        #real_class = y.iloc[idx]  # Actual class of the sample\n",
    "\n",
    "        real_class=y[idx]\n",
    "        bmu_idx = infer_som(new_data, weights)\n",
    "\n",
    "        bmu_indices.append(bmu_idx)\n",
    "        sample_classes.append(real_class)\n",
    "\n",
    "    # Visualize the Kohonen map with the BMUs of the samples\n",
    "    visualize_bmu_multiple(bmu_indices, weights, sample_classes)\n",
    "\n",
    "    # Convert bmu_indices to a format compatible with accuracy_score\n",
    "    # This is a simplified approach; a more robust solution would involve mapping BMUs to predicted classes\n",
    "    predicted_classes = [0] * len(bmu_indices) # Placeholder: Needs a proper mapping from BMU to class\n",
    "\n",
    "    print(\"Accuracy: \",accuracy_score(y, predicted_classes))\n",
    "\n",
    "\n",
    "# Function to test inference on random samples\n",
    "def test_inference_on_random_samples(n_samples, X, y, weights):\n",
    "    # Select n random indices from the dataset\n",
    "    sample_indices = np.random.choice(range(len(X)), size=n_samples, replace=False)\n",
    "    bmu_indices = []\n",
    "    sample_classes = []\n",
    "    # For each sample, perform inference and store the BMUs and classes\n",
    "    for idx in sample_indices:\n",
    "        new_data = X[idx]\n",
    "        real_class = y[idx]  # Actual class of the sample\n",
    "        bmu_idx = infer_som(new_data, weights)\n",
    "\n",
    "        bmu_indices.append(bmu_idx)\n",
    "        sample_classes.append(real_class)\n",
    "\n",
    "    # Visualize the Kohonen map with the BMUs of the samples\n",
    "    visualize_bmu_multiple(bmu_indices, weights, sample_classes)\n",
    "\n",
    "    # Convert bmu_indices to a format compatible with accuracy_score\n",
    "    # This is a simplified approach; a more robust solution would involve mapping BMUs to predicted classes\n",
    "    predicted_classes = [0] * len(bmu_indices) # Placeholder: Needs a proper mapping from BMU to class\n",
    "\n",
    "    print(\"Accuracy: \",accuracy_score(y, predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIKr130vEbfi"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "def build_bmu_class_map(X, y, weights):\n",
    "    bmu_class_map = defaultdict(list)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        bmu = infer_som(X[i], weights)\n",
    "        # Check if y is a pandas Series and use .iloc, otherwise use direct indexing\n",
    "        if isinstance(y, pd.Series):\n",
    "            bmu_class_map[tuple(bmu)].append(y.iloc[i])\n",
    "        else: # Assume it's a numpy array or similar\n",
    "            bmu_class_map[tuple(bmu)].append(y[i])\n",
    "\n",
    "\n",
    "    # Atribui a classe mais comum (votação majoritária) a cada BMU\n",
    "    bmu_to_class = {bmu: Counter(classes).most_common(1)[0][0]\n",
    "                    for bmu, classes in bmu_class_map.items()}\n",
    "\n",
    "    return bmu_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Exb8nn_SnWBs"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test_inference2(X, y, weights, bmu_to_class):\n",
    "    bmu_indices = []\n",
    "    sample_classes = []\n",
    "    predicted_classes = []\n",
    "\n",
    "    for idx in range(len(X)):\n",
    "        new_data = X[idx]\n",
    "        real_class = y[idx]\n",
    "\n",
    "        bmu_idx = infer_som(new_data, weights)\n",
    "        pred_class = bmu_to_class.get(tuple(bmu_idx), -1)  # -1 se BMU nunca foi ativado antes\n",
    "\n",
    "        bmu_indices.append(bmu_idx)\n",
    "        sample_classes.append(real_class)\n",
    "        predicted_classes.append(pred_class)\n",
    "\n",
    "    # Visualizar (opcional)\n",
    "    visualize_bmu_multiple(bmu_indices, weights, sample_classes)\n",
    "\n",
    "    # Calcular acurácia (ignorando casos -1 se quiser)\n",
    "    valid_idx = [i for i in range(len(predicted_classes)) if predicted_classes[i] != -1]\n",
    "    if valid_idx:\n",
    "        acc = accuracy_score(np.array(y)[valid_idx], np.array(predicted_classes)[valid_idx])\n",
    "        print(\"Accuracy:\", acc)\n",
    "    else:\n",
    "        print(\"Nenhum BMU conhecido foi ativado.\")\n",
    "\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXl6ggRRe5c7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jm193lylW6RL"
   },
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features (4 dimensions)\n",
    "y = data.target  # Classes\n",
    "# fit scaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,test_size=0.05 )\n",
    "print(\"Shape de X (features):\", X_train.shape)\n",
    "print(\"Shape de y (rótulo):\", y_train.shape)\n",
    "\n",
    "print(\"Shape de X (features):\", X_test.shape)\n",
    "print(\"Shape de y (rótulo):\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8qzyTMub0Bm"
   },
   "outputs": [],
   "source": [
    "# leitura de dados BankNote\n",
    "data_file = \"/content/drive/MyDrive/Colab_Notebooks/Kohonen/banknote_authentication2.xlsx\"\n",
    "\n",
    "# Read the Excel file using pandas\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "# Extract data into NumPy arrays\n",
    "X = df.iloc[:, :-1].values # Features (all columns except the last one)\n",
    "y = df.iloc[:, -1].values  # Labels (the last column)\n",
    "\n",
    "# fit scaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,test_size=0.05 )\n",
    "#train_x, test_x, y_train, y_test = train_test_split(X, y, stratify=y,test_size=0.05 )\n",
    "print(\"Shape do DataFrame completo:\", df.shape)\n",
    "print(\"Shape de X (features):\", X_train.shape)\n",
    "print(\"Shape de y (rótulo):\", y_train.shape)\n",
    "\n",
    "print(\"Shape do DataFrame completo:\", df.shape)\n",
    "print(\"Shape de X (features):\", X_test.shape)\n",
    "print(\"Shape de y (rótulo):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XJoxyXEEsSF"
   },
   "source": [
    "Pode-se observar claramente dois grupos distintos no gráfico de ACP, mas Iris Versicolor (classe 1) e Iris Virginica (classe 2) parecem bastante semelhantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSvlDppJFCpU"
   },
   "source": [
    "Tamanho do mapa : Pode escolheruma grade de neurônios de 10x10 para ter resolução suficiente para capturar as nuances nos dados.\n",
    "\n",
    "\n",
    "Número de características : Cada neurônio tem um vetor de peso de 4 dimensões correspondente às quatro características das flores de íris.\n",
    "\n",
    "Taxa de aprendizagem inicial : Uma taxa de aprendizagem de 0,5 permite ajustes significativos no início do treinamento.\n",
    "\n",
    "Raio inicial : definido como metade da maior dimensão do mapa, ou seja, 5 neste caso\n",
    "\n",
    "Raio inicial : definido como metade da maior dimensão do mapa, ou seja, 5 neste caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABEtwTxj4Q25"
   },
   "outputs": [],
   "source": [
    "#### Configurações\n",
    "# Map and data dimensions\n",
    "map_width = 10\n",
    "map_height = 10\n",
    "n_features = X_train.shape[1]  # 8 features\n",
    "print(\"Núnmero de atributos:\", n_features)\n",
    "# Random initialization of neuron weights\n",
    "weights = np.random.rand(map_width, map_height, n_features)\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate = 0.4\n",
    "n_iterations = 20000\n",
    "initial_radius = max(map_width, map_height) / 2\n",
    "time_constant = n_iterations / np.log(initial_radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0voifbVFs72"
   },
   "source": [
    "A função GAussiana é interessante porque combina uma zona de excitação (neurônios próximos à melhor neurônio são fortemente influenciados) com uma zona de inibição (neurônios ligeiramente mais distantes são menos influenciados), o que pode melhorar a convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leTp7OOf3_zE"
   },
   "outputs": [],
   "source": [
    "# Function to visualize the data after PCA\n",
    "def visualize_data_pca(X, y):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette=\"viridis\", s=100, edgecolor='k')\n",
    "    plt.title(\"Iris Data Visualization After PCA (2D)\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend(title=\"Classes\", loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to visualize the original data\n",
    "visualize_data_pca(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol9ajGKQGOnd"
   },
   "source": [
    "Raio de decaimento : o raio da vizinhança diminui ao longo do tempo, permitindo um ajuste fino.  \n",
    "\n",
    "Atualizações condicionais : somente neurônios dentro do raio atual são atualizados.\n",
    "\n",
    "pesos: A magnitude da atualização depende da distância do melhor neurônio e da forma da função do chapéu mexicano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGwPFX-B4ZiB"
   },
   "outputs": [],
   "source": [
    "# Training the KSOM with the Mexican Hat function\n",
    "for iteration in range(n_iterations):\n",
    "    # Select a random input vector\n",
    "    input_vector = X_train[np.random.randint(0, X_train.shape[0])]\n",
    "\n",
    "    # Find the Best Matching Unit (Best neuron)\n",
    "    bmu_idx = find_bmu(input_vector, weights)\n",
    "\n",
    "    # Update the weights of the BMU and neighboring neurons\n",
    "    weights = update_weights_mexican_hat(input_vector, bmu_idx, iteration, weights, map_width, map_height, learning_rate, time_constant)\n",
    "\n",
    "    # Gradually reduce the learning rate over iterations\n",
    "    learning_rate = 0.5 * np.exp(-iteration / n_iterations)\n",
    "    if iteration%1000==0:\n",
    "      print(\"--------\",iteration,\"--------\")\n",
    "      visualize_som_weights(weights)\n",
    "\n",
    "    # Call the function to visualize the map after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91e7834a"
   },
   "outputs": [],
   "source": [
    "bmu_to_class = build_bmu_class_map(X_train, y_train, weights)\n",
    "print(\"BMU to Class Mapping:\", bmu_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktf3KdIE4jWT"
   },
   "outputs": [],
   "source": [
    "# Example usage: Test inference on 20 random samples\n",
    "print(\"Shape de X (features):\", X_test.shape)\n",
    "print(\"Shape de y (rótulo):\", y_test.shape)\n",
    "predicted = test_inference2(X_test, y_test, weights, bmu_to_class)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNTAxJ4cpDcSsX9x36qUkrO",
   "mount_file_id": "1ZbN2Elht_CoxVaVb3kPR6UL5bQSdJuFX",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1ZbN2Elht_CoxVaVb3kPR6UL5bQSdJuFX",
     "timestamp": 1751057159104
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
